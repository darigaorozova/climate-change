import os
import sys
from datetime import date
import pandas as pd 
import findspark

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, round, xxhash64, date_format, 
    year, month, dayofmonth, hour, dayofweek, quarter
)
from clickhouse_driver import Client

# –ò–º–ø–æ—Ä—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –º–æ–¥—É–ª—è warehouse
# (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç –ª–µ–∂–∏—Ç –≤ –∫–æ—Ä–Ω–µ –∏–ª–∏ —Ä—è–¥–æ–º —Å –ø–∞–ø–∫–æ–π warehouse)
try:
    from config import db_config
except ImportError:
    # –ü–æ–ø—ã—Ç–∫–∞ –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å, –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫ –Ω–µ –∏–∑ –∫–æ—Ä–Ω—è
    sys.path.append(os.getcwd())
    try:
        from config import db_config
    except ImportError:
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª warehouse/config.py")
        sys.exit(1)


# --- –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ü–ê–ö–ï–¢–ù–û–ô –í–°–¢–ê–í–ö–ò ---
def insert_in_batches(client, df, table_name, batch_size=50000):
    """
    –ß–∏—Ç–∞–µ—Ç Spark DataFrame –∏ –≤—Å—Ç–∞–≤–ª—è–µ—Ç –≤ ClickHouse —á–∞—Å—Ç—è–º–∏.
    –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å (–∏–∑–±–µ–≥–∞–µ—Ç OutOfMemoryError).
    """
    print(f"üöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞–∫–µ—Ç–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –≤ {table_name}...")
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ (–¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞)
    total_rows = df.count()
    print(f"üìä –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {total_rows}")
    
    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –∏—Ç–µ—Ä–∞—Ç–æ—Ä (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
    iterator = df.toLocalIterator()
    
    batch = []
    count = 0
    batch_counter = 0

    for row in iterator:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É Spark –≤ —Å–ª–æ–≤–∞—Ä—å Python
        batch.append(row.asDict())
        
        # –ï—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º
        if len(batch) >= batch_size:
            client.execute(f'INSERT INTO {table_name} VALUES', batch)
            count += len(batch)
            batch_counter += 1
            print(f"   -> –ü–∞–∫–µ—Ç {batch_counter}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {count} / {total_rows} ({(count/total_rows)*100:.1f}%)")
            batch = [] # –û—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
    if batch:
        client.execute(f'INSERT INTO {table_name} VALUES', batch)
        count += len(batch)
        print(f"   -> –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–∫–µ—Ç: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {count} / {total_rows}")

    print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ {table_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

def process_and_load(output_dir):
    print(f"--- –ó–ê–ü–£–°–ö SPARK STAR SCHEMA ETL –î–õ–Ø {output_dir} ---")

    if not os.path.exists(output_dir):
        print(f"‚ùå –û—à–∏–±–∫–∞: –ü–∞–ø–∫–∞ {output_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return
    
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark
    spark = SparkSession.builder \
        .appName("WeatherETL_Batch") \
        .master("local[*]") \
        .config("spark.driver.memory", "4g") \
        .getOrCreate()
    
    # –†–∞–±–æ—Ç–∞–µ–º —Å—Ç—Ä–æ–≥–æ –≤ UTC
    spark.conf.set("spark.sql.session.timeZone", "UTC")
    spark.sparkContext.setLogLevel("WARN")

    try:
        # 2. –ß—Ç–µ–Ω–∏–µ Parquet —Ñ–∞–π–ª–æ–≤
        print("–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
        df_instant = spark.read.parquet(os.path.join(output_dir, "data_stream-oper_stepType-instant.parquet"))
        df_accum = spark.read.parquet(os.path.join(output_dir, "data_stream-oper_stepType-accum.parquet"))
        df_avg = spark.read.parquet(os.path.join(output_dir, "data_stream-oper_stepType-avg.parquet"))
        df_max = spark.read.parquet(os.path.join(output_dir, "data_stream-oper_stepType-max.parquet"))

        # 3. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ (JOIN)
        join_keys = ["time", "latitude", "longitude"]
        full_df = df_instant \
            .join(df_accum, on=join_keys, how="inner") \
            .join(df_avg, on=join_keys, how="inner") \
            .join(df_max, on=join_keys, how="inner")

        # 4. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
        print("–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        processed_df = full_df \
            .withColumn("time_id", date_format(col("time"), "yyyyMMddHH").cast("long")) \
            .withColumn("location_id", xxhash64(col("latitude"), col("longitude"))) \
            .withColumn("temperature_c", round(col("t2m") - 273.15, 2)) \
            .withColumn("dewpoint_c", round(col("d2m") - 273.15, 2)) \
            .withColumn("max_temp_c", round(col("mx2t") - 273.15, 2)) \
            .withColumn("min_temp_c", round(col("mn2t") - 273.15, 2)) \
            .withColumn("pressure_hpa", round(col("msl") / 100, 2)) \
            .withColumn("precipitation_mm", round(col("tp") * 1000, 4)) \
            .withColumn("wind_speed_ms", round((col("u10")**2 + col("v10")**2)**0.5, 2)) \
            .withColumnRenamed("avg_sdswrf", "solar_radiation") \
            .withColumnRenamed("tcc", "cloud_cover")

        # --- 5. –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê –¢–ê–ë–õ–ò–¶–´ ---

        # --- 5. –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê –¢–ê–ë–õ–ò–¶–´ (–°—Ö–µ–º–∞ –ó–≤–µ–∑–¥–∞) ---
        # A. DIM_TIME (–ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏)
        dim_time_df = processed_df.select(
            col("time_id"),
            date_format(col("time"), "yyyy-MM-dd HH:mm:ss").alias("timestamp"), # <-- –°—Ç—Ä–æ–∫–∞ –≤–º–µ—Å—Ç–æ Timestamp
            year(col("time")).alias("year"),
            month(col("time")).alias("month"),
            dayofmonth(col("time")).alias("day"),
            hour(col("time")).alias("hour"),
            # –§–æ—Ä–º—É–ª–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: Spark(1=Sun) -> ISO(7=Sun)
            ((dayofweek(col("time")) + 5) % 7 + 1).alias("day_of_week"),
            quarter(col("time")).alias("quarter")
        ).distinct()

        # B. DIM_LOCATION
        dim_location_df = processed_df.select(
            col("location_id"),
            col("latitude"),
            col("longitude")
        ).distinct()

        # C. FACT_WEATHER
        fact_df = processed_df.select(
            "time_id", "location_id",
            "temperature_c", "dewpoint_c",
            "max_temp_c", "min_temp_c",
            "pressure_hpa", "precipitation_mm",
            "wind_speed_ms", "cloud_cover", "solar_radiation"
        )

        # --- 6. –ó–ê–ì–†–£–ó–ö–ê –í CLICKHOUSE ---
        print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse ({db_config['host']}:{db_config['port']})...")
        client = Client(**db_config)

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ DIM_LOCATION
        # –õ–æ–∫–∞—Ü–∏–π –º–∞–ª–æ (1500 —à—Ç), –º–æ–∂–Ω–æ –≥—Ä—É–∑–∏—Ç—å —Å—Ä–∞–∑—É —á–µ—Ä–µ–∑ Pandas
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ dim_location ({dim_location_df.count()} –∑–∞–ø–∏—Å–µ–π)...")
        client.execute('INSERT INTO weather_db.dim_location VALUES', dim_location_df.toPandas().to_dict('records'))

        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ DIM_TIME
        # –í—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –º–∞–ª–æ (24 * 31 = 744 —à—Ç), –≥—Ä—É–∑–∏–º —á–µ—Ä–µ–∑ Pandas —Å —Ñ–∏–∫—Å–æ–º –¥–∞—Ç—ã
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ dim_time ({dim_time_df.count()} –∑–∞–ø–∏—Å–µ–π)...")
        pdf_dim_time = dim_time_df.toPandas()
        pdf_dim_time['timestamp'] = pd.to_datetime(pdf_dim_time['timestamp']) # –§–∏–∫—Å –¥–ª—è –¥—Ä–∞–π–≤–µ—Ä–∞
        client.execute('INSERT INTO weather_db.dim_time VALUES', pdf_dim_time.to_dict('records'))

        # 3. –ó–∞–≥—Ä—É–∑–∫–∞ FACT_WEATHER
        # –§–∞–∫—Ç–æ–≤ –û–ß–ï–ù–¨ –º–Ω–æ–≥–æ (>1 –º–ª–Ω), –∏—Å–ø–æ–ª—å–∑—É–µ–º BATCH INSERT
        insert_in_batches(client, fact_df, "weather_db.fact_weather", batch_size=50000)

        print("\n‚úÖ ETL –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Spark ETL: {e}")
        import traceback
        traceback.print_exc()
    finally:
        spark.stop()

if __name__ == "__main__":

    year_list = ["2022", "2023", "2024", "2025"]
    month_list = ["01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"]

    for yea in year_list:
        for mont in month_list:
            print(f"--- –ó–ê–ü–£–°–ö: {yea}_{mont} ---")

            # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
            dir = os.path.join("raw_data", f"{yea}_{mont}")

            process_and_load(dir)